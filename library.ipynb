{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bFEVtmPNL224"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution():\n",
        "  def __init__(self,filter_size):\n",
        "    self.filter_height,self.filter_width=filter_size\n",
        "    self.padding=1\n",
        "    self.stride=1\n",
        "\n",
        "  def get_patches(self,image_array,pool=False):\n",
        "    self.image_array=image_array\n",
        "    print(self.image_array.shape)\n",
        "    self.batch_size,self.image_height,self.image_width= self.image_array.shape\n",
        "    if pool==True:\n",
        "      self.stride=2\n",
        "      self.filter_height=2\n",
        "      self.filter_width=2\n",
        "    else:\n",
        "      self.stride=1\n",
        "      self.filter_height=3\n",
        "      self.filter_width=3\n",
        "    self.new_height=(self.image_height-self.filter_height)//self.stride + 1\n",
        "    self.new_width=(self.image_width-self.filter_width)//self.stride + 1\n",
        "    self.new_shape=(self.batch_size,self.new_height,self.new_width,self.filter_height,self.filter_width)\n",
        "    self.mem_location=(self.image_array.strides[0],self.image_array.strides[1]*self.stride,self.image_array.strides[2]*self.stride,self.image_array.strides[1],self.image_array.strides[2])\n",
        "    self.patches=as_strided(self.image_array,self.new_shape,self.mem_location)\n",
        "    return self.patches\n",
        "\n",
        "  def forward(self,image_array):\n",
        "    self.image_array=image_array\n",
        "    self.image_array_padded=np.pad(self.image_array,((0,0),(self.padding,self.padding),(self.padding,self.padding)),mode='constant')\n",
        "    print(f'image_array_padded{self.image_array_padded.shape}')\n",
        "    self.patches=self.get_patches(self.image_array_padded)\n",
        "    self.patches_reshaped=self.patches.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2],self.filter_height*self.filter_width)\n",
        "    print(f'patches{self.patches.shape}')\n",
        "    print(f'patches_reshaped{self.patches_reshaped.shape}')\n",
        "    self.filter_vertical=np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
        "    self.filter_vertical=self.filter_vertical.reshape(9,1)\n",
        "    self.filter_horizontal=np.array([[-1,-2,-1],[0,0,0],[1,2,1]]) #self initialized sobel filter because its simple cnn task so why waste computation\n",
        "    self.filter_horizontal=self.filter_vertical.reshape(9,1)\n",
        "    #print(f'patches{self.patches.shape}')\n",
        "    print(f'patches_reshaped{self.patches_reshaped.shape}')\n",
        "    self.edge_vertical=np.tensordot(self.patches_reshaped,self.filter_vertical,axes=([3],[0]))\n",
        "    self.edge_horizontal=np.tensordot(self.patches_reshaped,self.filter_horizontal,axes=([3],[0]))\n",
        "    self.edge=np.sqrt(np.square(self.edge_vertical)+np.square(self.edge_horizontal))\n",
        "    self.edge=self.edge.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2])\n",
        "    self.patches2=self.get_patches(self.edge,pool=True)  #say poolsize is different but wanna use only one function\n",
        "    print(f'patches2{self.patches2.shape}')\n",
        "    #self.patches2=self.patches2.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2],9)\n",
        "    self.patches2=self.patches2.reshape(self.patches.shape[0],self.patches.shape[1],self.patches.shape[2],self.patches.shape[3]*self.patches.shape[4])\n",
        "\n",
        "    #print(f'patches2{self.patches2.shape}')\n",
        "    # print(f'patches{self.patches[0][0][0]}')\n",
        "    #print(f'patches ko shape{self.patches2.shape}')\n",
        "\n",
        "\n",
        "    self.output=np.max(self.patches2,axis=3)\n",
        "    #print(f'output of pool{self.output.shape}')\n",
        "    # self.output=self.output.reshape(self.batch_size,self.output_height,self.output_width)\n",
        "    del self.patches, self.patches_reshaped, self.edge, self.patches2,self.filter_vertical,self.filter_horizontal,self.edge_vertical,self.edge_horizontal\n",
        "\n",
        "    return self.output.reshape(self.output.shape[0],-1)"
      ],
      "metadata": {
        "id": "nMyefyeQNtuZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Dense():\n",
        "    def __init__(self, ninputs, nnodes,activation=None ):\n",
        "\n",
        "        self.weight = np.random.randn(ninputs, nnodes) * np.sqrt(2. / ninputs) #xaiver initialization\n",
        "        self.bias = np.random.rand(nnodes) * 0.01\n",
        "        self.sdw = np.zeros((ninputs, nnodes))\n",
        "        self.sdb = np.zeros(nnodes)\n",
        "        self.vdw = np.zeros((ninputs, nnodes))\n",
        "        self.vdb = np.zeros(nnodes)\n",
        "        self.t = 0\n",
        "        self.activation=activation\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.dot(inputs, self.weight) + self.bias\n",
        "        if self.activation:\n",
        "          self.output=self.activation.forward(self.output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        if self.activation:\n",
        "          gradient=self.activation.backward(gradient)\n",
        "        self.gradient_weight = np.dot(self.input.T, gradient)\n",
        "        self.gradient_bias = np.sum(gradient, axis=0)\n",
        "        self.gradient_input = np.dot(gradient, self.weight.T)\n",
        "\n",
        "\n",
        "        return self.gradient_input\n",
        "\n",
        "    def calculate(self, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.t += 1\n",
        "            beta1, beta2 = 0.9, 0.999\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            self.sdw = beta2 * self.sdw + (1 - beta2) * (self.gradient_weight ** 2)\n",
        "            self.sdb = beta2 * self.sdb + (1 - beta2) * (self.gradient_bias ** 2)\n",
        "\n",
        "            self.vdw = beta1 * self.vdw + (1 - beta1) * self.gradient_weight\n",
        "            self.vdb = beta1 * self.vdb + (1 - beta1) * self.gradient_bias\n",
        "\n",
        "            # Bias correction for adam optimizer for the starting difference while using exponantially weighted average\n",
        "            sdw_corrected = self.sdw / (1 - beta2 ** self.t)\n",
        "            sdb_corrected = self.sdb / (1 - beta2 ** self.t)\n",
        "            vdw_corrected = self.vdw / (1 - beta1 ** self.t)\n",
        "            vdb_corrected = self.vdb / (1 - beta1 ** self.t)\n",
        "\n",
        "            self.sdw_corrected = sdw_corrected\n",
        "            self.sdb_corrected = sdb_corrected\n",
        "            self.vdw_corrected = vdw_corrected\n",
        "            self.vdb_corrected = vdb_corrected\n",
        "\n",
        "    def update(self, learning_rate, optimizer):\n",
        "        if optimizer == 'adam':\n",
        "            self.weight -= learning_rate * self.vdw_corrected / (np.sqrt(self.sdw_corrected) + 1e-8)\n",
        "            self.bias -= learning_rate * self.vdb_corrected / (np.sqrt(self.sdb_corrected) + 1e-8)\n",
        "        else:\n",
        "            self.weight -= learning_rate * self.gradient_weight\n",
        "            self.bias -= learning_rate * self.gradient_bias\n",
        "\n",
        "    def l2(self):\n",
        "        return np.sum(self.weight ** 2)\n",
        "\n",
        "class Relu():\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        self.gradient = gradients * (self.input > 0) #why not self.output>>>because we need a boolean return\n",
        "        return self.gradient\n",
        "\n",
        "\n",
        "class Softmax():\n",
        "    def __init__(self,final=False):\n",
        "        self.final = final\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradient):\n",
        "      if self.final == True:\n",
        "        return gradient\n",
        "      else:\n",
        "        self.dinputs = gradient * self.output * (1 - self.output)  # Derivative of softmax\n",
        "        return self.dinputs\n",
        "\n",
        "class CategoricalCrossEntropyLoss():\n",
        "    def forward(self, probs, true_outputs, layers,lamda=0):\n",
        "        clipped_probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
        "        loss_data = -np.sum(true_outputs * np.log(clipped_probs)) / (len(true_outputs) + 1e-8)\n",
        "\n",
        "        # l2_terms = [lamda * np.sum(layer.l2()) for layer in layers]\n",
        "        # loss_weight = 0.5 * np.sum(l2_terms) / (len(true_outputs) +  1e-8)\n",
        "        return loss_data\n",
        "\n",
        "    def accuracy(self, probs, true_outputs):\n",
        "\n",
        "        prediction=np.argmax(probs, axis=1)\n",
        "        true_label=np.argmax(true_outputs, axis=1)\n",
        "        accuracy=np.mean(prediction == true_label)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, probs, true_outputs):\n",
        "        samples = len(true_outputs)\n",
        "\n",
        "        self.dinputs = (probs - true_outputs) / samples\n",
        "        return self.dinputs\n",
        "\n"
      ],
      "metadata": {
        "id": "oMATxzp_qN2s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "class NeuralNetwork():\n",
        "  def __init__(self,loss_function='CategoricalCrossEntropyLoss()',optimizer='adam',learning_rate=0.001):\n",
        "     self.conv=[]\n",
        "     self.conv2=[]\n",
        "     self.layers=[]\n",
        "     self.loss_function = loss_function\n",
        "     self.learning_rate = learning_rate\n",
        "     self.optimizer = optimizer\n",
        "\n",
        "\n",
        "  def add(self,layer,grad=True):\n",
        "    if grad==True:\n",
        "      self.layers.append(layer)\n",
        "    else:\n",
        "      self.conv.append(layer)\n",
        "      self.conv2.append(layer)\n",
        "\n",
        "\n",
        "  def fit(self, X_train, y_train,batch_size,epochs=10):\n",
        "      self.epochs=epochs\n",
        "      for convu in self.conv:\n",
        "          X_train= convu.forward(X_train)\n",
        "      for epoch in range(self.epochs):\n",
        "          epoch_loss = 0\n",
        "          epoch_loss_val = 0\n",
        "          for i in range(0, len(X_train), batch_size):\n",
        "              batch_inputs = X_train[i:i + batch_size]\n",
        "\n",
        "              batch_true_outputs = y_train[i:i + batch_size]\n",
        "\n",
        "\n",
        "              x = batch_inputs\n",
        "\n",
        "              #print(f'x ko shape{x.shape}')\n",
        "              for layer in self.layers:\n",
        "                  x = layer.forward(x)\n",
        "                  #print(x.shape)\n",
        "\n",
        "\n",
        "              loss = self.loss_function.forward(x, batch_true_outputs, self.layers)\n",
        "              epoch_loss += loss  # Accumulate batch loss\n",
        "\n",
        "              gradient = self.loss_function.backward(x, batch_true_outputs)\n",
        "              for layer in reversed(self.layers):\n",
        "                  # print(f'gradient is {gradient.shape}')\n",
        "\n",
        "\n",
        "                  gradient = layer.backward(gradient)\n",
        "                  # print(f'gradient is {gradient.shape}')\n",
        "\n",
        "              for layer in self.layers:\n",
        "\n",
        "                layer.calculate(self.optimizer)\n",
        "\n",
        "              for layer in self.layers:\n",
        "\n",
        "                layer.update(self.learning_rate, self.optimizer)\n",
        "\n",
        "\n",
        "          print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(X_train) * batch_size}\")  # Print average loss for the epoch\n",
        "          epoch_accuracy = 0\n",
        "          epoch_loss_val = 0\n",
        "          # for i in range(0,len(X_test),batch_size):\n",
        "          #     batch_validate = X_test[i:i + batch_size]\n",
        "          #     batch_validate_true = y_test[i:i + batch_size]\n",
        "\n",
        "  def eval(self,X_test,y_test):\n",
        "          x2=X_test\n",
        "          for convu in self.conv2:\n",
        "            x2=convu.forward(x2)\n",
        "          for layer in self.layers:\n",
        "              x2=layer.forward(x2)\n",
        "\n",
        "          loss_validate = self.loss_function.forward(x2, y_test, self.layers)\n",
        "          accurate=self.loss_function.accuracy(x2, y_test)\n",
        "\n",
        "          print(f\"val_Loss: {loss_validate},val_accuracy:{accurate}\")\n",
        "\n",
        "\n",
        "  def save_model(self, filename):\n",
        "        \"\"\"Saves the current model to a file using pickle.\"\"\"\n",
        "        with open(filename, 'wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "        print(f\"Model saved to {filename}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def load_model(filename):\n",
        "        \"\"\"Loads a model from a file using pickle.\"\"\"\n",
        "        with open(filename, 'rb') as file:\n",
        "            model = pickle.load(file)\n",
        "        print(f\"Model loaded from {filename}\")\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "Cy5eZmAyqgUS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0J-j5RB3c4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}